{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Daily Papers and Create a Dataset\n",
    "This notebook aims to gather the daily papers listed in Hugging Face and create a dataset that can later be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arxiv pypdf scholarly tqdm huggingface_hub pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from scholarly import scholarly\n",
    "import pprint\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hugging_face_top_daily_paper(url) -> list:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the most upvoted paper on Hugging Face daily papers.\n",
    "    It returns a list of papers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the title element from the JSON-like data in the \"data-props\" attribute\n",
    "        containers = soup.find_all('div', class_='SVELTE_HYDRATER contents')\n",
    "        paper_list = []\n",
    "\n",
    "        for container in containers:\n",
    "            # Parse the data-props attribute if it exists\n",
    "            if 'data-props' in container.attrs:\n",
    "                try:\n",
    "                    # Find all article elements that contain paper information\n",
    "                    articles = container.find_all('article')\n",
    "                    \n",
    "                    for article in articles:\n",
    "                        # Find the paper title within the article\n",
    "                        title_element = article.find('h3')\n",
    "                        if title_element:\n",
    "                            title = title_element.find('a').text.strip()\n",
    "                            paper_list.append(title)\n",
    "                except:\n",
    "                    continue\n",
    "        return paper_list\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while fetching the HTML: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_past_year_urls():\n",
    "    end_date = datetime(2025, 2, 10)\n",
    "    start_date = datetime(2023, 5, 10)\n",
    "    current_date = start_date\n",
    "    urls = []\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        url = f\"https://huggingface.co/papers?date={current_date.strftime('%Y-%m-%d')}\"\n",
    "        urls.append(url)\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get papers for each day in the past year\n",
    "papers_by_date = {}\n",
    "for url in get_past_year_urls():\n",
    "    # Extract date from URL\n",
    "    date_str = url.split('date=')[1]\n",
    "    papers = get_hugging_face_top_daily_paper(url)\n",
    "    if papers:\n",
    "        papers_by_date[date_str] = papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days: 643\n",
      "Total number of papers: 7791\n",
      "\n",
      "Sample of the data:\n",
      "\n",
      "Date: 2023-05-10\n",
      "Papers:\n",
      "- To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review\n",
      "- Recommender Systems with Generative Retrieval\n",
      "- Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Print number of days and total papers\n",
    "print(f\"Number of days: {len(papers_by_date)}\")\n",
    "total_papers = sum(len(papers) for papers in papers_by_date.values())\n",
    "print(f\"Total number of papers: {total_papers}\")\n",
    "\n",
    "# Print a sample (first day)\n",
    "print(\"\\nSample of the data:\")\n",
    "sample_date = next(iter(papers_by_date))\n",
    "print(f\"\\nDate: {sample_date}\")\n",
    "print(\"Papers:\")\n",
    "for paper in papers_by_date[sample_date][:3]:  # print first 3 papers\n",
    "    print(f\"- {paper}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have listed all the top papers in the given timeframe.\n",
    "\n",
    "Next is to get details about each paper and its authors.\n",
    "\n",
    "## Get Data for Each Paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from scholarly import scholarly\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "def get_paper_by_title(paper_title: str):\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(query=paper_title)))\n",
    "    return paper\n",
    "\n",
    "def download_paper_by_id(paper_id: str) -> None:\n",
    "    \"\"\"\n",
    "    This tool gets the id of a paper and downloads it from arxiv. It saves the paper locally\n",
    "    in the current directory as \"paper.pdf\".\n",
    "\n",
    "    Args:\n",
    "        paper_id: The id of the paper to download.\n",
    "    \"\"\"\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "    paper.download_pdf(filename=\"paper.pdf\")\n",
    "    return None\n",
    "\n",
    "def get_author_information_by_name(name: str):\n",
    "    search_query = scholarly.search_author(name)\n",
    "    author = scholarly.fill(next(search_query))\n",
    "    return author\n",
    "\n",
    "def get_paper_id_by_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the arxiv paper id by its title.\n",
    "    It returns the title of the paper\n",
    "\n",
    "    Args:\n",
    "        title: The paper title for which to get the id.\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    papers = api.list_papers(query=title)\n",
    "    if papers:\n",
    "        for paper in papers:\n",
    "            if paper.title == title:\n",
    "                return paper.id\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"entry_id\": \"http://arxiv.org/abs/2304.09355v5\",\n",
      "    \"updated\": \"2023-11-21 13:12:21+00:00\",\n",
      "    \"published\": \"2023-04-19 00:33:59+00:00\",\n",
      "    \"title\": \"To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review\",\n",
      "    \"authors\": [\n",
      "        \"Ravid Shwartz-Ziv\",\n",
      "        \"Yann LeCun\"\n",
      "    ],\n",
      "    \"summary\": \"Deep neural networks excel in supervised learning tasks but are constrained\\nby the need for extensive labeled data. Self-supervised learning emerges as a\\npromising alternative, allowing models to learn without explicit labels.\\nInformation theory, and notably the information bottleneck principle, has been\\npivotal in shaping deep neural networks. This principle focuses on optimizing\\nthe trade-off between compression and preserving relevant information,\\nproviding a foundation for efficient network design in supervised contexts.\\nHowever, its precise role and adaptation in self-supervised learning remain\\nunclear. In this work, we scrutinize various self-supervised learning\\napproaches from an information-theoretic perspective, introducing a unified\\nframework that encapsulates the \\\\textit{self-supervised information-theoretic\\nlearning problem}. We weave together existing research into a cohesive\\nnarrative, delve into contemporary self-supervised methodologies, and spotlight\\npotential research avenues and inherent challenges. Additionally, we discuss\\nthe empirical evaluation of information-theoretic quantities and their\\nestimation methods. Overall, this paper furnishes an exhaustive review of the\\nintersection of information theory, self-supervised learning, and deep neural\\nnetworks.\",\n",
      "    \"comment\": null,\n",
      "    \"journal_ref\": null,\n",
      "    \"doi\": null,\n",
      "    \"primary_category\": \"cs.LG\",\n",
      "    \"categories\": [\n",
      "        \"cs.LG\",\n",
      "        \"cs.IT\",\n",
      "        \"math.IT\"\n",
      "    ],\n",
      "    \"links\": [\n",
      "        \"http://arxiv.org/abs/2304.09355v5\",\n",
      "        \"http://arxiv.org/pdf/2304.09355v5\"\n",
      "    ],\n",
      "    \"pdf_url\": \"http://arxiv.org/pdf/2304.09355v5\",\n",
      "    \"_raw\": {\n",
      "        \"id\": \"http://arxiv.org/abs/2304.09355v5\",\n",
      "        \"guidislink\": true,\n",
      "        \"link\": \"http://arxiv.org/abs/2304.09355v5\",\n",
      "        \"updated\": \"2023-11-21T13:12:21Z\",\n",
      "        \"updated_parsed\": [\n",
      "            2023,\n",
      "            11,\n",
      "            21,\n",
      "            13,\n",
      "            12,\n",
      "            21,\n",
      "            1,\n",
      "            325,\n",
      "            0\n",
      "        ],\n",
      "        \"published\": \"2023-04-19T00:33:59Z\",\n",
      "        \"published_parsed\": [\n",
      "            2023,\n",
      "            4,\n",
      "            19,\n",
      "            0,\n",
      "            33,\n",
      "            59,\n",
      "            2,\n",
      "            109,\n",
      "            0\n",
      "        ],\n",
      "        \"title\": \"To Compress or Not to Compress- Self-Supervised Learning and Information\\n  Theory: A Review\",\n",
      "        \"title_detail\": {\n",
      "            \"type\": \"text/plain\",\n",
      "            \"language\": null,\n",
      "            \"base\": \"\",\n",
      "            \"value\": \"To Compress or Not to Compress- Self-Supervised Learning and Information\\n  Theory: A Review\"\n",
      "        },\n",
      "        \"summary\": \"Deep neural networks excel in supervised learning tasks but are constrained\\nby the need for extensive labeled data. Self-supervised learning emerges as a\\npromising alternative, allowing models to learn without explicit labels.\\nInformation theory, and notably the information bottleneck principle, has been\\npivotal in shaping deep neural networks. This principle focuses on optimizing\\nthe trade-off between compression and preserving relevant information,\\nproviding a foundation for efficient network design in supervised contexts.\\nHowever, its precise role and adaptation in self-supervised learning remain\\nunclear. In this work, we scrutinize various self-supervised learning\\napproaches from an information-theoretic perspective, introducing a unified\\nframework that encapsulates the \\\\textit{self-supervised information-theoretic\\nlearning problem}. We weave together existing research into a cohesive\\nnarrative, delve into contemporary self-supervised methodologies, and spotlight\\npotential research avenues and inherent challenges. Additionally, we discuss\\nthe empirical evaluation of information-theoretic quantities and their\\nestimation methods. Overall, this paper furnishes an exhaustive review of the\\nintersection of information theory, self-supervised learning, and deep neural\\nnetworks.\",\n",
      "        \"summary_detail\": {\n",
      "            \"type\": \"text/plain\",\n",
      "            \"language\": null,\n",
      "            \"base\": \"\",\n",
      "            \"value\": \"Deep neural networks excel in supervised learning tasks but are constrained\\nby the need for extensive labeled data. Self-supervised learning emerges as a\\npromising alternative, allowing models to learn without explicit labels.\\nInformation theory, and notably the information bottleneck principle, has been\\npivotal in shaping deep neural networks. This principle focuses on optimizing\\nthe trade-off between compression and preserving relevant information,\\nproviding a foundation for efficient network design in supervised contexts.\\nHowever, its precise role and adaptation in self-supervised learning remain\\nunclear. In this work, we scrutinize various self-supervised learning\\napproaches from an information-theoretic perspective, introducing a unified\\nframework that encapsulates the \\\\textit{self-supervised information-theoretic\\nlearning problem}. We weave together existing research into a cohesive\\nnarrative, delve into contemporary self-supervised methodologies, and spotlight\\npotential research avenues and inherent challenges. Additionally, we discuss\\nthe empirical evaluation of information-theoretic quantities and their\\nestimation methods. Overall, this paper furnishes an exhaustive review of the\\nintersection of information theory, self-supervised learning, and deep neural\\nnetworks.\"\n",
      "        },\n",
      "        \"authors\": [\n",
      "            {\n",
      "                \"name\": \"Ravid Shwartz-Ziv\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Yann LeCun\"\n",
      "            }\n",
      "        ],\n",
      "        \"author_detail\": {\n",
      "            \"name\": \"Yann LeCun\"\n",
      "        },\n",
      "        \"author\": \"Yann LeCun\",\n",
      "        \"links\": [\n",
      "            {\n",
      "                \"href\": \"http://arxiv.org/abs/2304.09355v5\",\n",
      "                \"rel\": \"alternate\",\n",
      "                \"type\": \"text/html\"\n",
      "            },\n",
      "            {\n",
      "                \"title\": \"pdf\",\n",
      "                \"href\": \"http://arxiv.org/pdf/2304.09355v5\",\n",
      "                \"rel\": \"related\",\n",
      "                \"type\": \"application/pdf\"\n",
      "            }\n",
      "        ],\n",
      "        \"arxiv_primary_category\": {\n",
      "            \"term\": \"cs.LG\",\n",
      "            \"scheme\": \"http://arxiv.org/schemas/atom\"\n",
      "        },\n",
      "        \"tags\": [\n",
      "            {\n",
      "                \"term\": \"cs.LG\",\n",
      "                \"scheme\": \"http://arxiv.org/schemas/atom\",\n",
      "                \"label\": null\n",
      "            },\n",
      "            {\n",
      "                \"term\": \"cs.IT\",\n",
      "                \"scheme\": \"http://arxiv.org/schemas/atom\",\n",
      "                \"label\": null\n",
      "            },\n",
      "            {\n",
      "                \"term\": \"math.IT\",\n",
      "                \"scheme\": \"http://arxiv.org/schemas/atom\",\n",
      "                \"label\": null\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sample_paper_details = get_paper_by_title(papers_by_date['2023-05-10'][0])\n",
    "print(json.dumps(sample_paper_details.__dict__, indent=4 , default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_arxiv_id(entry_id: str) -> str:\n",
    "    \"\"\"Extract arXiv ID from entry_id URL using regex.\"\"\"\n",
    "    match = re.search(r'abs/([0-9]+\\.[0-9]+)', entry_id)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:   6%|▋         | 501/7791 [00:14<02:54, 41.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  13%|█▎        | 1001/7791 [00:56<05:11, 21.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  19%|█▉        | 1501/7791 [01:53<06:28, 16.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  26%|██▌       | 2001/7791 [02:53<07:57, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  32%|███▏      | 2501/7791 [04:26<16:52,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  39%|███▊      | 3001/7791 [05:47<06:52, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  45%|████▍     | 3500/7791 [19:02<2:21:17,  1.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  51%|█████▏    | 4000/7791 [29:48<2:08:24,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  58%|█████▊    | 4500/7791 [43:54<1:48:17,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  64%|██████▍   | 5000/7791 [57:55<1:22:11,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  71%|███████   | 5500/7791 [1:11:21<1:18:20,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  77%|███████▋  | 6000/7791 [1:25:07<1:07:28,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  82%|████████▏ | 6388/7791 [1:34:24<58:20,  2.50s/it]  Bozo feed; consider handling: document declared as utf-8, but parsed as windows-1252\n",
      "Processing papers:  83%|████████▎ | 6500/7791 [1:38:04<30:56,  1.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  90%|█████████ | 7043/7791 [2:25:36<07:08,  1.75it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  97%|█████████▋| 7564/7791 [2:25:39<00:03, 67.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at checkpoint 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 7791/7791 [2:25:40<00:00,  1.12s/it] \n"
     ]
    }
   ],
   "source": [
    "json_file = 'Assets/papers_details.json'\n",
    "title_id_file = 'Assets/paper_title_id.json'\n",
    "papers_with_details = {}\n",
    "paper_title_id = {}\n",
    "\n",
    "# Load existing data if files exist\n",
    "if os.path.exists(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        papers_with_details = json.load(f)\n",
    "\n",
    "if os.path.exists(title_id_file):\n",
    "    with open(title_id_file, 'r') as f:\n",
    "        paper_title_id = json.load(f)\n",
    "\n",
    "# Calculate total number of papers\n",
    "total_papers = sum(len(papers) for papers in papers_by_date.values())\n",
    "counter = 0\n",
    "\n",
    "# Create progress bar\n",
    "with tqdm(total=total_papers, desc=\"Processing papers\") as pbar:\n",
    "    for date, papers in papers_by_date.items():\n",
    "        for paper in papers:\n",
    "            try:\n",
    "                # Save every 500 papers\n",
    "                if counter % 500 == 0 and counter != 0:\n",
    "                    # Truncate and write new data\n",
    "                    try:\n",
    "                        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "                            f.truncate(0)\n",
    "                            json.dump(papers_with_details, f, indent=4, default=str, ensure_ascii=False)\n",
    "                            f.flush()\n",
    "                            os.fsync(f.fileno())\n",
    "                        with open(title_id_file, 'w', encoding='utf-8') as f:\n",
    "                            f.truncate(0)\n",
    "                            json.dump(paper_title_id, f, indent=4, ensure_ascii=False)\n",
    "                            f.flush() \n",
    "                            os.fsync(f.fileno())\n",
    "                        print(f\"saved at checkpoint {counter//500}.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving files: {e}\")\n",
    "                        \n",
    "                # Skip if paper title already exists\n",
    "                if paper in paper_title_id:\n",
    "                    # print(\"skipped\")\n",
    "                    pbar.update(1)\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                    \n",
    "                paper_details = get_paper_by_title(paper)\n",
    "                # Extract paper ID from entry_id URL\n",
    "                paper_id = extract_arxiv_id(paper_details.entry_id)\n",
    "                \n",
    "                # Skip if paper already exists in JSON\n",
    "                if paper_id in papers_with_details:\n",
    "                    pbar.update(1)\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                    \n",
    "                papers_with_details[paper_id] = paper_details.__dict__\n",
    "                paper_title_id[paper] = paper_id\n",
    "                \n",
    "                \n",
    "                counter += 1\n",
    "                pbar.update(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pbar.update(1)\n",
    "                counter += 1\n",
    "                continue\n",
    "\n",
    "# Save final results\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(papers_with_details, f, indent=4, default=str)\n",
    "with open(title_id_file, 'w') as f:\n",
    "    json.dump(paper_title_id, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the papers details from JSON file\n",
    "with open('Assets/papers_details.json', 'r', encoding='utf-8') as f:\n",
    "    papers_with_details = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entry_id': 'http://arxiv.org/abs/2304.09355v5',\n",
       " 'updated': '2023-11-21 13:12:21+00:00',\n",
       " 'published': '2023-04-19 00:33:59+00:00',\n",
       " 'title': 'To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review',\n",
       " 'authors': ['Ravid Shwartz-Ziv', 'Yann LeCun'],\n",
       " 'summary': 'Deep neural networks excel in supervised learning tasks but are constrained\\nby the need for extensive labeled data. Self-supervised learning emerges as a\\npromising alternative, allowing models to learn without explicit labels.\\nInformation theory, and notably the information bottleneck principle, has been\\npivotal in shaping deep neural networks. This principle focuses on optimizing\\nthe trade-off between compression and preserving relevant information,\\nproviding a foundation for efficient network design in supervised contexts.\\nHowever, its precise role and adaptation in self-supervised learning remain\\nunclear. In this work, we scrutinize various self-supervised learning\\napproaches from an information-theoretic perspective, introducing a unified\\nframework that encapsulates the \\\\textit{self-supervised information-theoretic\\nlearning problem}. We weave together existing research into a cohesive\\nnarrative, delve into contemporary self-supervised methodologies, and spotlight\\npotential research avenues and inherent challenges. Additionally, we discuss\\nthe empirical evaluation of information-theoretic quantities and their\\nestimation methods. Overall, this paper furnishes an exhaustive review of the\\nintersection of information theory, self-supervised learning, and deep neural\\nnetworks.',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'doi': None,\n",
       " 'primary_category': 'cs.LG',\n",
       " 'categories': ['cs.LG', 'cs.IT', 'math.IT'],\n",
       " 'links': ['http://arxiv.org/abs/2304.09355v5',\n",
       "  'http://arxiv.org/pdf/2304.09355v5'],\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2304.09355v5',\n",
       " '_raw': {'id': 'http://arxiv.org/abs/2304.09355v5',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2304.09355v5',\n",
       "  'updated': '2023-11-21T13:12:21Z',\n",
       "  'updated_parsed': [2023, 11, 21, 13, 12, 21, 1, 325, 0],\n",
       "  'published': '2023-04-19T00:33:59Z',\n",
       "  'published_parsed': [2023, 4, 19, 0, 33, 59, 2, 109, 0],\n",
       "  'title': 'To Compress or Not to Compress- Self-Supervised Learning and Information\\n  Theory: A Review',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'To Compress or Not to Compress- Self-Supervised Learning and Information\\n  Theory: A Review'},\n",
       "  'summary': 'Deep neural networks excel in supervised learning tasks but are constrained\\nby the need for extensive labeled data. Self-supervised learning emerges as a\\npromising alternative, allowing models to learn without explicit labels.\\nInformation theory, and notably the information bottleneck principle, has been\\npivotal in shaping deep neural networks. This principle focuses on optimizing\\nthe trade-off between compression and preserving relevant information,\\nproviding a foundation for efficient network design in supervised contexts.\\nHowever, its precise role and adaptation in self-supervised learning remain\\nunclear. In this work, we scrutinize various self-supervised learning\\napproaches from an information-theoretic perspective, introducing a unified\\nframework that encapsulates the \\\\textit{self-supervised information-theoretic\\nlearning problem}. We weave together existing research into a cohesive\\nnarrative, delve into contemporary self-supervised methodologies, and spotlight\\npotential research avenues and inherent challenges. Additionally, we discuss\\nthe empirical evaluation of information-theoretic quantities and their\\nestimation methods. Overall, this paper furnishes an exhaustive review of the\\nintersection of information theory, self-supervised learning, and deep neural\\nnetworks.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Deep neural networks excel in supervised learning tasks but are constrained\\nby the need for extensive labeled data. Self-supervised learning emerges as a\\npromising alternative, allowing models to learn without explicit labels.\\nInformation theory, and notably the information bottleneck principle, has been\\npivotal in shaping deep neural networks. This principle focuses on optimizing\\nthe trade-off between compression and preserving relevant information,\\nproviding a foundation for efficient network design in supervised contexts.\\nHowever, its precise role and adaptation in self-supervised learning remain\\nunclear. In this work, we scrutinize various self-supervised learning\\napproaches from an information-theoretic perspective, introducing a unified\\nframework that encapsulates the \\\\textit{self-supervised information-theoretic\\nlearning problem}. We weave together existing research into a cohesive\\nnarrative, delve into contemporary self-supervised methodologies, and spotlight\\npotential research avenues and inherent challenges. Additionally, we discuss\\nthe empirical evaluation of information-theoretic quantities and their\\nestimation methods. Overall, this paper furnishes an exhaustive review of the\\nintersection of information theory, self-supervised learning, and deep neural\\nnetworks.'},\n",
       "  'authors': [{'name': 'Ravid Shwartz-Ziv'}, {'name': 'Yann LeCun'}],\n",
       "  'author_detail': {'name': 'Yann LeCun'},\n",
       "  'author': 'Yann LeCun',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2304.09355v5',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2304.09355v5',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.LG',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.LG',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.IT', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None},\n",
       "   {'term': 'math.IT',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_with_details['2304.09355']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save paper nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store paper data\n",
    "paper_data = []\n",
    "\n",
    "# Extract required attributes from each paper in the last 2000 papers\n",
    "for i, (paper_id, paper_details) in enumerate(papers_with_details.items(), 1):\n",
    "    paper_row = {\n",
    "        'node_id': f\"paper_{i}\",\n",
    "        'paper_id': paper_id,\n",
    "        'publish_date': paper_details['published'].split()[0],\n",
    "        'title': paper_details['title']\n",
    "    }\n",
    "    paper_data.append(paper_row)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(paper_data)\n",
    "df.to_csv('GraphDataset/paper.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Author and Organization Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique authors: 18522\n",
      "Top 5 authors by paper count:\n",
      "Yu Qiao: 37 papers\n",
      "Ziwei Liu: 34 papers\n",
      "Dahua Lin: 31 papers\n",
      "Hongsheng Li: 28 papers\n",
      "Lei Zhang: 25 papers\n"
     ]
    }
   ],
   "source": [
    "# Create a set to store unique authors and a counter dictionary\n",
    "unique_authors = set()\n",
    "author_counts = {}\n",
    "\n",
    "# Create lists to store author data and paper-author relationships\n",
    "author_data = []\n",
    "paper_author_data = []\n",
    "author_id = 1\n",
    "author_name_to_id = {}\n",
    "\n",
    "# Extract authors from each paper\n",
    "for i, (paper_id, paper_details) in enumerate(papers_with_details.items(), 1):\n",
    "    if 'authors' in paper_details:\n",
    "        for author in paper_details['authors']:\n",
    "            author_name = author\n",
    "            if author_name not in unique_authors:\n",
    "                # Add new author\n",
    "                unique_authors.add(author_name)\n",
    "                author_data.append({\n",
    "                    'node_id': f\"author_{author_id}\",\n",
    "                    'name': author_name\n",
    "                })\n",
    "                author_name_to_id[author_name] = f\"author_{author_id}\"\n",
    "                author_id += 1\n",
    "                author_counts[author_name] = 1\n",
    "            else:\n",
    "                # Increment count for existing author\n",
    "                author_counts[author_name] += 1\n",
    "            \n",
    "            # Add paper-author relationship\n",
    "            paper_author_data.append({\n",
    "                'paper_id': f\"paper_{i}\",\n",
    "                'author_id': author_name_to_id[author_name]\n",
    "            })\n",
    "\n",
    "# Convert to DataFrames and save as CSV\n",
    "author_df = pd.DataFrame(author_data)\n",
    "author_df.to_csv('GraphDataset/author.csv', index=False)\n",
    "\n",
    "paper_author_df = pd.DataFrame(paper_author_data)\n",
    "paper_author_df.to_csv('GraphDataset/paper_author.csv', index=False)\n",
    "\n",
    "print(f\"Total unique authors: {len(unique_authors)}\")\n",
    "print(f\"Top 5 authors by paper count:\")\n",
    "top_authors = sorted(author_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for author, count in top_authors:\n",
    "    print(f\"{author}: {count} papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique categories: 224\n",
      "Top 5 categories by paper count:\n",
      "cs.CV: 1832 papers\n",
      "cs.CL: 1710 papers\n",
      "cs.AI: 1561 papers\n",
      "cs.LG: 1325 papers\n",
      "cs.RO: 209 papers\n"
     ]
    }
   ],
   "source": [
    "# Create a set to store unique categories and a counter dictionary\n",
    "unique_categories = set()\n",
    "category_counts = {}\n",
    "\n",
    "# Create lists to store category data and paper-category relationships\n",
    "category_data = []\n",
    "paper_category_data = []\n",
    "category_id = 1\n",
    "category_name_to_id = {}\n",
    "\n",
    "# Extract categories from each paper\n",
    "for i, (paper_id, paper_details) in enumerate(papers_with_details.items(), 1):\n",
    "    if 'categories' in paper_details:\n",
    "        for category in paper_details['categories']:\n",
    "            if category not in unique_categories:\n",
    "                # Add new category\n",
    "                unique_categories.add(category)\n",
    "                category_data.append({\n",
    "                    'node_id': f\"category_{category_id}\",\n",
    "                    'name': category\n",
    "                })\n",
    "                category_name_to_id[category] = f\"category_{category_id}\"\n",
    "                category_id += 1\n",
    "                category_counts[category] = 1\n",
    "            else:\n",
    "                # Increment count for existing category\n",
    "                category_counts[category] += 1\n",
    "            \n",
    "            # Add paper-category relationship\n",
    "            paper_category_data.append({\n",
    "                'paper_id': f\"paper_{i}\",\n",
    "                'category_id': category_name_to_id[category]\n",
    "            })\n",
    "\n",
    "# Convert to DataFrames and save as CSV\n",
    "category_df = pd.DataFrame(category_data)\n",
    "category_df.to_csv('GraphDataset/category.csv', index=False)\n",
    "\n",
    "paper_category_df = pd.DataFrame(paper_category_data)\n",
    "paper_category_df.to_csv('GraphDataset/paper_category.csv', index=False)\n",
    "\n",
    "print(f\"\\nTotal unique categories: {len(unique_categories)}\")\n",
    "print(f\"Top 5 categories by paper count:\")\n",
    "top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for category, count in top_categories:\n",
    "    print(f\"{category}: {count} papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Summary:\n",
      "Total papers: 4193\n",
      "Total unique categories: 224\n",
      "Total unique authors: 18522\n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(f\"Total papers: {len(papers_with_details)}\")\n",
    "print(f\"Total unique categories: {len(unique_categories)}\")\n",
    "\n",
    "# Count unique authors\n",
    "unique_authors = set()\n",
    "for paper_details in papers_with_details.values():\n",
    "    if 'authors' in paper_details:\n",
    "        for author in paper_details['authors']:\n",
    "            if isinstance(author, dict):\n",
    "                unique_authors.add(author['name'])\n",
    "            elif isinstance(author, str):\n",
    "                unique_authors.add(author)\n",
    "\n",
    "print(f\"Total unique authors: {len(unique_authors)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
